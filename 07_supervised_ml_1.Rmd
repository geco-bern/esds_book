# Supervised Machine Learning I {#ch-06}

## Introduction

### Learning objectives

After this learning unit, you will be able to ...

-   Differentiate machine learning from classical statistical modelling
-   Describe the different variants of machine learning
-   Conceptualize model training as an optimization problem
-   Describe overfitting and how it can be measured (training vs. validation error).
-   Formulate a model in R.
-   Discuss why, when, and how to pre-process data.
-   Measure and minimize loss for regression and classification (video)
-   Describe the fundamentals of gradient descent (video)

### Important points from the lecture

Machine learning refers to a class of algorithms that automatically generate statistical models of data. There are two main types of machine learning:

**Unsupervised machine learning**: A class of algorithms that automatically detect patterns in data without using labels or 'learning without a teacher'. Examples are: PCAs, k-means clustering, autoencoders, self-organizing maps, etc.

**Supervised machine learning**: A class of algorithms that automatically learn an input-output relationships based on example input-output pairs. Examples include: support vector machines, random forests, decision trees, neural networks, etc. Supervised machine learning requires three ingredients: (1) Input data, (2) Output data, and (3) A measure of model performance (a.k.a. "loss"). Supervised machine learning be used for regression (predict a continuous label) or classification (predict a categorical label).

**Loss** is a concept central to many supervised machine learning algorithms. It measures how well our predicted model values fit the actual observed model values. During training, the machine learning algorithm optimizes the loss. Typically, this is a minimization of some error metric, e.g. the mean absolute error or the root mean square error.

## Tutorial

### Overfitting {#overfitting}

Machine learning (ML) may appear magical. The ability of ML algorithms to detect patterns and make predictions is fascinating. However, several challenges have to be met in the process of formulating, training, and evaluating the models. In this practical we will discuss some basics of supervised ML and how to achieve best predictive results.

In general, the aim of supervised ML is to find a model $\hat{Y} = f(X)$ that is *trained* (calibrated) using observed relationships between a set of *features* (also known as *predictors*, or *labels*, or *independent variables*) $X$ and the *target* variable $Y$. Note, that $Y$ is observed. The hat on $\hat{Y}$ indicates that it is an estimate, provided by the model $f$ . Some algorithms can even handle predictions of multiple target variables simultaneously (e.g., neural networks). ML algorithms consist of (more or less) flexible mathematical models with a certain structure and set of parameters. At the simple extreme end of the model spectrum is the univariate linear regression. You may not want to call this a ML algorithm because there is no iterative learning involved in fitting a linear regression model. Nevertheless, also a univariate linear regression provides a prediction $\hat{Y} = f(X)$, just like other (proper) ML algorithms do.

The functional form of a univariate linear regression is not particularly flexible (just a straight line for the best fit between predictors and targets) and it has only two parameters (slope and intercept). At the other extreme end are, for example, deep neural networks. They are extremely flexible, can learn highly non-linear relationships and deal with interactions between a large number of predictors. They also contain very large numbers of parameters, typically on the order of thousands. You can imagine that this allows these types of algorithms to very effectively learn from the data, but also bears the risk of *overfitting*.

What is overfitting? The following example illustrates it. Let's assume that there is some true underlying relationship between a predictor $x$ and the target variable $y$. We don't know this relationship (in the code below, this is `true_fun()`) and the observations contain a (normally distributed) error (`y = true_fun(x) + 0.1 * rnorm(n_samples)`). Based on our training data (`df_train`), we fit polynomial models of degree 1, 4, and 15 to the observations. A polynomial of degree N is given by: $$
y = \sum_{n=0}^N a_n x^n
$$ $a_n$ are the coefficients, i.e., model parameters. The goal of the training is to get the coefficients $a_n$. From the above definition, the polynomial of degree 15 has 16 parameters, while the polynomial of degree 1 has two parameters (and corresponds to a simple linear regression). You can imagine that the polynomial of degree 15 is much more flexible and should thus yield the closest fit to the training data. This is indeed the case.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(modelr)
library(forcats)
library(yardstick)

true_fun <- function(x){
  cos(1.5 * pi * x)
}

set.seed(2)

n_samples <- 30

# create training data
df_train <- tibble( x = runif(n_samples, min = 0, max = 1)) %>%
  mutate(y = true_fun(x) + 0.1 * rnorm(n_samples)) %>%
  arrange(x)

polyfit_1 <- lm(y ~ poly(x, 1), data = df_train)
polyfit_4 <- lm(y ~ poly(x, 4), data = df_train)
polyfit_15 <- lm(y ~ poly(x, 15), data = df_train)

## XXX Add this: performance::check_model()

# create a function that takes the data set (here training) and the models and returns the evaluation results (plot with annotated RMSE)
eval_fits <- function(df, polyfit_1, polyfit_4, polyfit_15, justdata = FALSE){

  # training results
  df <- df %>%
    rename(y_true = y) %>%
    add_predictions(polyfit_1, var = "poly1") %>%
    add_predictions(polyfit_4, var = "poly4") %>%
    add_predictions(polyfit_15, var = "poly15")

  ## at equally spaced x
  df_fit <- tibble(x = seq(from = min(df$x), to = max(df$x), length.out = 100)) %>%
    add_predictions(polyfit_1, var = "poly1") %>%
    add_predictions(polyfit_4, var = "poly4") %>%
    add_predictions(polyfit_15, var = "poly15") %>%
    pivot_longer(cols = starts_with("poly"), names_to = "fit", values_to = "y_pred") %>%
    mutate(fit = fct_relevel(fit, "poly1", "poly4", "poly15"))

  ## get a table (data frame) for the RMSE
  df_metrics_train <- tibble(
    fittype = "poly1",
    rmse = metrics(df,
                   truth = y_true,
                   estimate = poly1
                   ) %>%
      filter(.metric == "rmse") %>%
      pull(.estimate)) %>%
    bind_rows(
      .,
      tibble(
       fittype = "poly4",
        rmse = metrics(df,
                   truth = y_true,
                   estimate = poly4
                   ) %>%
         filter(.metric == "rmse") %>%
         pull(.estimate))
    ) %>%
    bind_rows(
      .,
      tibble(
       fittype = "poly15",
        rmse = metrics(df,
                   truth = y_true,
                   estimate = poly15
                   ) %>%
         filter(.metric == "rmse") %>%
         pull(.estimate))
    )

  # plot training results
  if (justdata){
    gg <- ggplot() +
      geom_point(data = df, aes(x, y_true)) +
      ylim(-1.5, 1) +
      labs(y = "y")
    
  } else {
    gg <- ggplot() +
      geom_point(data = df, aes(x, y_true)) +
      geom_line(data = df_fit, aes(x = x, y = y_pred, color = fit)) +
      stat_function(fun = true_fun, linetype = "dotted") +
      ylim(-1.5, 1) +
      labs(
        subtitle = paste("RMSE: poly1 =", format(df_metrics_train$rmse[1], digits = 2), ", poly4 =", format(df_metrics_train$rmse[2], digits = 2), ", poly15 =", format(df_metrics_train$rmse[3], digits = 2)),
        y = "y")
    
  }

  return(gg)
}

# gg <- eval_fits(df_train, polyfit_1, polyfit_4, polyfit_15, justdata = TRUE)
# gg + labs(title = "The data")

gg <- eval_fits(df_train, polyfit_1, polyfit_4, polyfit_15)
gg + labs(title = "Training")
```

We can use the same fitted models on unseen data - the *validation data*. This is what's done below. Again, the same true underlying relationship is used, but we sample a new set of data points in x and add a new sample of errors on top of the true relationship.

```{r  echo=FALSE, message=FALSE, warning=FALSE}
set.seed(1)

# create testing data
df_test <- tibble( x = runif(n_samples, min = 0, max = 1)) %>%
  mutate(y = true_fun(x) + 0.1 * rnorm(n_samples)) %>%
  arrange(x)

gg <- eval_fits(df_test, polyfit_1, polyfit_4, polyfit_15)
gg + labs(title = "Validation")
```

You see that, using the validation set, we find that "poly4" actually performs the best - it has a much lower RMSE that "poly15". Apparently, "poly15" was overfitted. Apparently, it indeed used its flexibility to fit not only the shape of the true underlying relationship, but also the observation errors on top of it. This has obviously the implication that, when this model is used to make predictions for data that was not used for training (calibration), it will yield misguided predictions that are affected by the errors in the training set. In the above pictures we can also conclude that "poly1" was underfitted.

It gets even worse when applying the fitted polynomial models to data that extends beyond the range in $x$ that was used for model training. Here, we're extending just 10% to the left and to the right.

```{r  echo=FALSE, message=FALSE, warning=FALSE}
set.seed(1)

# create testing data
df_test <- tibble( x = runif(n_samples, min = -0.1, max = 1.1)) %>%
  mutate(y = true_fun(x) + 0.1 * rnorm(n_samples)) %>%
  arrange(x)

gg <- eval_fits(df_test, polyfit_1, polyfit_4, polyfit_15)
gg + labs(title = "Validation (with extrapolation)")

# ggsave("fig/overfitting_demo_polynomial.pdf", width = 6, height = 4)
```

You see that the RMSE for "poly15" literally explodes. The model is hopelessly overfitted and completely useless for prediction, although it looked like it fit the data best when we considered at the training results. This is a fundamental challenge in ML - finding the model with the best *generalisability*. That is, a model that not only fits the training data well, but also performs well on unseen data.

The phenomenon of fitting/overfitting as a function of the model "flexibility" is also referred to as *bias vs. variance trade-off*. The bias describes how well a model matches the training set (average error). A model with low bias will match the data set closely and vice versa. The variance describes how much a model changes when you train it using different portions of your data set. "poly15" has a high variance, but much of its variance is the result of misled training on observation errors. On the other extreme, "poly1" has a high bias. It's not affected by the noise in observations, but its predictions are also far off the observations. In ML, we are challenged to balance this trade-off.

The next chapters introduce methods to achieve the best model generalisability and to find the sweet spot between high bias and high variance. The steps to get there include the splitting of data into training and testing sets, pre-processing of data and model training which "steers" the model towards what is considered a good model fit in terms of its generalisation power.

You have learned in video 6a about the basic setup of supervised ML, with input data containing the features (or predictors) $X$, predicted ($\hat{Y}$) and observed target values ($Y$, also known as *labels*). In video 6b on loss and it's minimization, you learned about the loss function which quantifies the agreement between $Y$ and $\hat{Y}$ and defines the objective of the model training. Here, you'll learn how all of this can be implemented in R. Depending on your application or research question, it may also be of interest to evaluate the relationships embodied in $f(X)$ or to quantify the importance of different predictors in our model. This is referred to as *model interpretation* and is introduced in the respectively named subsection. Finally, we'll get into *feature selection* in the next Application session.

The topic of supervised machine learning methods covers enough material to fill two sessions. Therefore, we split this part in two. Model training, implementing the an entire modelling workflow, model evaluation and interpretation will be covered in the next session's tutorial (Supervised Machine Learning Methods II).

Of course, a plethora of algorithms exist that do the job of $Y = f(X)$. Each of them has its own strengths and limitations. It is beyond the scope of this course to introduce a larger set of algorithms. Subsequent sessions will focus primarily on Artificial Neural Networks (ANN) - a type of ML algorithm that has gained popularity for its capacity to efficiently learn patterns in large data sets. For illustration purposes in this and the next chapter, we will briefly introduce three relatively simple alternative "ML" methods, linear regression, K-nearest-neighbors, and Random Forest (RF). They have quite different characteristics and are therefore great for illustration purposes in this chapter. RF has become very popular for a wide variety of applications in environmental sciences. It is a particularly "user-friendly" algorithm as it performs well "out-of-the-box".

### Modelling challenge {#motivation}

In this tutorial, we formulate a model for predicting ecosystem gross primary production (photosynthesis) from environmental covariates. This is to say that `GPP_NT_VUT_REF` is the target variable, and other available variables available in the dataset from Chapter \@ref(2) can be used as predictors.

Abiotic factors largely determine ecosystem-atmosphere exchange fluxes of water vapour and CO2. Temporally changing mass exchange fluxes can be continuously measured with the eddy covariance technique, while abiotic variables (meteorological variables, soil moisture) can be measured in parallel. This offers an opportunity for building models that predict mass exchange fluxes from a set of abiotic predictors.

Data is provided here at daily resolution for a site ('CH-Dav') located in the Swiss alps (Davos). This is one of the longest-running eddy covariance sites globally and measures fluxes in an evergreen coniferous forest with cold winters and temperate, relatively moist summers.

For more information of the variables in the dataset, see the [FLUXNET 2015 website](http://fluxnet.fluxdata.org/data/fluxnet2015-dataset/), and [Pastorello et al., 2020](https://www.nature.com/articles/s41597-020-0534-3) for a comprehensive documentation of variable definitions and methods.

**Available variables are:**

-   `TIMESTAMP`: Day of measurement.
-   `TA_F`: Air temperature. The meaning of suffix `_F` is described in [Pastorello et al., 2020](https://www.nature.com/articles/s41597-020-0534-3).
-   `SW_IN_F`: Shortwave incoming radiation
-   `LW_IN_F`: Longwave incoming radiation
-   `VPD_F`: Vapour pressure deficit (relates to the humidity of the air)
-   `PA_F`: Atmospheric pressure
-   `P_F`: Precipitation
-   `WS_F`: Wind speed
-   `GPP_NT_VUT_REF`: Gross primary production - **the target variable**
-   `NEE_VUT_REF_QC`: Quality control information for `GPP_NT_VUT_REF`. Specifies the fraction of high-quality underlying high-frequency data from which the daily data is derived. 0.8 = 80% underlying high-quality data, remaining 20% of the high-frequency data is gap-filled.

### A selection of model types

#### Linear regression

The simplest form of a linear model (LM) is the univariate linear regression, where we assume a linear relationship between $X$ and $Y$:

$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \;\;\; i = 1, 2, ...n \;,
$$

where $Y_i$ is the i-th observation of the target variable, and $X_i$ is the i-th value of the (single) predictor variable. The errors $\epsilon_i$ are assumed to be independent from each other (no autocorrelation), normally distributed, have mean of zero and a constant variance. $\beta_0$ and $\beta_1$ are constant coefficients (model parameters). Fitting a linear regression is finding the values for $\beta_0$ and $\beta_1$ so that the sum of the square errors is minimized, that is:

$$
\sum_i \epsilon_i^2 = \sum_i (Y_i - \beta_0 - \beta_1 X_i)^2 = \text{argmin}.
$$

Since the expected value of $\epsilon$ is zero (because it's normally distributed with mean zero), predictions of a linear regression model are obtained by $Y_\text{new} = \beta_0 + \beta_1 X_\text{new}$, and provide unbiased estimates.

It's not hard to imagine that the univariate linear regression can be generalized to a multivariate linear regression, where we assume that the target variable is a linear combination of $p$ predictor variables: $$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \; ... \; + \beta_p X_p + \epsilon \;.
$$Note that here, $X$, $Y$, and $\epsilon$ are vectors of length corresponding to the number of observations in our data set ($n$ - as above). Analogously, calibrating the $p$ coefficients $\beta_1, \beta_2, ..., \beta_p$ is to minimize the sum of square errors $\sum_i \epsilon_i^2$. While the regression is a line in two-dimensional space for the univariate case, it is a plane in three-dimensional space for bi-variate regression, and so on.

#### K-nearest neighbours

As the name suggests, K-nearest neighbour (KNN) uses the $k$ observations that are "nearest" to the new record for which we want to make a prediction. It then calculates their average (in regression) or most frequent value (in classification) as the prediction. "Nearest" is determined by some distance metric evaluated based on the values of the predictors. In our example (`GPP_NT_VUT_REF ~ .`), KNN would determine the $k$ days where conditions, given by our set of predictors, were most similar (nearest) to the day for which we seek a prediction. Then, it calculates the prediction as the average (mean) GPP value of these days. Determining "nearest" neighbors is commonly based on either the Euclidean or Manhattan distances between two data points $x_a$ and $x_b$, considering all $p$ predictors $j$.

Euclidean distance: $$
\sqrt{ \sum_{j=1}^p (x_{a,j} - x_{b,j})^2  } \\
$$

Manhattan distance: $$
\sum_{j=1}^p | x_{a,j} - x_{b,j} |
$$

In two-dimensional space, the Euclidean distance measures the length of a straight line between two points (remember Pythagoras!). The Manhattan distance is called this way because it measures the distance you would have to walk to get from point $a$ to point $b$ in Manhattan, New York, where you cannot cut corners but have to follow a rectangular grid of streets. $|x|$ is the absolute (positive) value of $x$ ( $|-x| = x$).

KNN is a simple algorithm that uses knowledge of the "local" data structure for prediction. A drawback is that the model training has to be done for each prediction step and the computation time of the training increases with $n \times p$. KNNs are often used to impute values (fill missing values) and have the advantage that predicted values are always within the range of observed values of the target variable.

#### Random forest

Random forest (RF) models are based on *decision trees*, where binary decisions for predicting the target's values are based on thresholds of the predictors' values. The *depth* of a decision tree refers to the number of such decisions. The deeper a tree, the more likely the model will overfit.

Just as forests are made up by trees, *Random Forest* models make use of random subsets of the original data and of available predictions and respective decision trees. Predictions are then made by averaging predictions of individual *base learners* (the decision trees). The number of predictors considered at each decision step is a tunable parameter (a *hyperparameter*, typically called $m_{try}$). Introducing this randomness is effective because decision trees tend to overfit and because of the *wisdom of the crowd* - i.e., the power of aggregating individual predictions with their random error (and without systematic bias) for generating accurate and relatively precise predictions.

Random forest models have gained particular popularity and are widely applied in environmental sciences not only for their power, but also for their ease of use. No pre-processing (centering, scaling) is necessary, they can deal with skewed data, and can effectively learn interactions between predictors.

You can learn more on how random forests work in the book [Hands On Machine-Learning in R](https://bradleyboehmke.github.io/HOML/random-forest.html).

Before we move on to the actual implementation of LM, KNN and RF, we first have to understand how we can properly prepare our data in order to fit the respective model's requirements and how to validate model performance.

### Data splitting

#### Reading and wrangling data

There is a difference between data wrangling and pre-processing as part of the modelling workflow. *Data wrangling* can be considered to encompass the steps to prepare the data set *prior* to modelling, including, the combination of variables from different sources, removal of bad or missing data, and aggregating to the desired resolution or granularity (e.g., averaging over all time steps in a day, or over all replicates in a sample). See the [Quartz Guide to Bad Data](https://github.com/Quartz/bad-data-guide) for an overview of how to deal with different types of bad data.

In contrast, *data pre-processing* refers to the additional steps that are either required by the ML algorithm (e.g. centering and scaling for KNN or neural networks) or the transformation of variables guided by the resulting improvement of the predictive power of the ML model. In other words, pre-processing is part of the modelling workflow and includes all steps that apply transformations that use parameters derived from the data.

Let's read the data, select relevant variables, convert the time stamp column to a time object and interpret missing values (encoded `-9999` in the file).

```{r warning=FALSE, message=FALSE}
library(tidyverse)
ddf <- read_csv("./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") %>% 
  
  ## select only the variables we are interested in
  select(TIMESTAMP,
         GPP_NT_VUT_REF,    # the target
         NEE_VUT_REF_QC,    # quality control info
         ends_with("_F"),   # includes all all meteorological variables
         -contains("JSB")   # weird useless variable
         ) %>%

  ## convert to a nice date object
  mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) %>%

  ## set all -9999 to NA
  na_if(-9999) %>%

  ## drop QC variables (no longer needed), except NEE_VUT_REF_QC
  select(-ends_with("_QC"), NEE_VUT_REF_QC)
```

The column `NEE_VUT_REF_QC` provides information about the fraction of gap-filled half-hourly data used to calculate daily aggregates. Let's use only `GPP_NT_VUT_REF` data, where at least 80% of the underlying half-hourly data was good quality measured data, and not gap-filled. Make sure to not actually remove the respective rows, but rather replace values with NA.

```{r warning=FALSE, message=FALSE}
ddf <- ddf %>% 
  mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF))
```

At this stage, we won't use `NEE_VUT_REF_QC` any longer. So we can drop it.

```{r warning=FALSE, message=FALSE}
ddf <- ddf %>% 
  select(-NEE_VUT_REF_QC)
```

#### Splitting into testing and training sets

The introductory example impressively demonstrated the importance of validating the fitted model with data that was **not** used for training. Thus, we can test the model's *generalisability.* The essential step that enables us to assess the model's *generalization error* is to hold out part of the data from training, and set it aside (leaving it absolutely untouched) for *testing*.

There is no fixed rule for how much data are to be used for training and testing, respectively. We have to balance the trade-off between:

-   Spending too much data for training will leave us with too little data for testing and the test results may not be robust. In this case, the sample size for getting robust validation statistics is not sufficiently large and we don't know for sure whether we are safe from an over-fit model.
-   Spending too much data for validation will leave us with too little data for training. In this case, the ML algorithm may not be successful at finding real relationships due to insufficient amounts of training data.

Typical splits are between 60-80% for training. However, in cases where the number of data points is very large, the gains from having more training data are marginal but come at the cost of adding to the already high computational burden of model training.

In environmental sciences, the number of predictors is often smaller than the sample size ($p < n$), because it's typically easier to collect repeated observations of a particular variable than to expand the set of variables being observed. Nevertheless, in cases where the number $p$ gets large, it is important, and for some algorithms mandatory, to maintain $p < n$ for model training.

An important aspect to consider when splitting the data is to make sure that all "states" of the system for which we have data are approximately equally represented in training and testing sets. This is to make sure that the algorithm learns relationships $f(X)$ also under rare conditions $X$, for example meteorological extreme events. In other words, we want a *balanced* training set.

Several alternative functions for the data splitting step are available from different packages in R. We will use the the **rsample** package because it allows to additionally make sure that data from the full range of a given variable's values (`VPD_F` in the example below) are well covered in both training and testing sets.

```{r warning=FALSE, message=FALSE}
library(rsample)
set.seed(123)  # for reproducibility
split     <- initial_split(ddf, prop = 0.7, strata = "VPD_F")
ddf_train <- training(split)
ddf_test  <- testing(split)
```

Plot the distribution of values in the training and testing sets.

```{r warning=FALSE, message=FALSE}
ddf_train %>% 
  mutate(split = "train") %>% 
  bind_rows(ddf_test %>% 
    mutate(split = "test")) %>% 
  pivot_longer(cols = 2:9, names_to = "variable", values_to = "value") %>% 
  ggplot(aes(x = value, y = ..density.., color = split)) +
  geom_density() +
  facet_wrap(~variable, scales = "free")
```

### Pre-processing {#preprocessing}

#### The use of recipes

Skewed data, outliers, and values covering multiple orders of magnitude can create difficulties for certain ML algorithms, e.g., K-nearest neighbours or neural networks. Other algorithms, like tree-based methods (e.g., Random Forest), are more robust against such issues.

When defining any pre-processing step, it should be specified as a "recipe" or "blueprint", and not actually executed on the data itself before we start with the model training. Such a "recipe" can then be applied to any new data, while the parameters of the data pre-processing transformations are different each time. We will introduce an example below where we divide all values in the testing and training dataset by the standard deviation of the respective dataset. Note that, because the two datasets hold different values, they also have different standard deviations. The same "recipe" (i.e., division by standard deviation) will be applied to both datasets but the transformation is done with a different parameter (i.e., the different standard deviations in either dataset).

The reason for not actually executing the same data transformation with the same parameters on both dataset is the risk of *data leakage*. This happens when information from the validation data somehow finds its way into the training step. Don't worry if this sounds incomprehensible. We'll learn more about it below. For now, you can focus on the types of pre-processing steps, what they do, and how they are implemented as part of a pre-processing "recipe" in R.

Many pre-processing steps can be done either by hand or using different R packages. Below, we give you a mix of both approaches to emphasize on how pre-processing is done. The [recipes](https://recipes.tidymodels.org/) package offers a powerful way to specify pre-processing steps in R and is gaining traction as part of the [tidymodels](https://www.tidymodels.org/) ecosystem. Moreover, it is compatible with the [caret](https://topepo.github.io/caret/) package that will be used to formulate the different ML models.

#### Target engineering [BONUS]

Target engineering refers to pre-processing of the target variable. Its application can enable improved predictions, particularly for models that make assumptions about errors (e.g., normally distributed errors in linear regression) and when the target variable follows a "special" distribution (e.g., heavily skewed distribution, or where the target variable is a fraction that is naturally bounded by 0 and 1).

A simple log-transformation of the target variable can often resolve issues with skewed distributions. An implication of a log-transformation is that errors in predicting values in the upper end of the observed range do not affect the model disproportionately compared to errors in the lower range.

In our data set of half-hourly ecosystem flux and meteorological measurements, the variable `WS_F` (wind speed) is skewed. The target variable that we have considered so far (`GPP_NT_VUT_REF`) is not skewed. In a case where we would consider `WS_F` to be our target variable, we would thus consider applying a log-transformation.

```{r message=F, warning=F}
library(patchwork)  # to combine two plots into separate panels of a single plot

gg1 <- ddf_train %>% 
  ggplot(aes(x = WS_F, y = ..density..)) +
  geom_histogram() +
  labs(title = "Original")

gg2 <- ddf_train %>% 
  ggplot(aes(x = log(WS_F), y = ..density..)) +
  geom_histogram() +
  labs(title = "Log-transformed 'by hand'")

gg1 + gg2   # the + is from the patchwork library
```

Now, how can we do this using the *recipes* package? The syntax is that we define a model with target and predictor variables, the data the recipe should be applied to, and the steps of the recipe:

```{r}
library(recipes)
recipe_example <- recipe(WS_F ~ ., data = ddf) %>% 
  step_log(all_outcomes())
recipe_example
```

The formula, specified as an argument in the `recipe()` function, defines variable "roles" (in recipes-speak). What's left of the `~` is interpreted as an "outcome" (the target variable). By writing `~ .`, we specify all remaining variables in the data frame as "predictors". As an argument in the `step_log()` function, we write `all_outcomes()` to declare that the log transformation is applied only to the "outcome" (target) variable.

The output of `recipe_example` tells us that the recipe holds a log-transformation on all outcome variables (here, 1). As it is in the real world, writing a recipe does not mean to actually cook it, respectively in the *recipes* terminology to bake it. There are two more steps involved to get there. This might seem a nuisance at first but their separation is actually quite beautiful and translates the conception of the pre-processing as a "blueprint" into the way we write the code. You'll understand why this is so useful throughout the next Chapters.

The full routine from writing the recipe to preparing and baking it is as follows:

```{r}
## First, we have to prepare everything, taking the prepared recipe and our ingredients (data)
prep_example <- prep(recipe_example, training = ddf_train)

## Second, we have to bake our mixture, to get a nice meal 
df_baked <- bake(prep_example, new_data = ddf_train)
```

The log-transformed data from the recipe-routine should look the same as when simply using the *log()* command as was done above.

```{r message=FALSE}
gg3 <- df_baked %>% 
  ggplot(aes(x = WS_F, y = ..density..)) +
  geom_histogram() +
  labs(title = "log-transformed 'by recipe'")

(gg1 + gg2) / (plot_spacer() + gg3)
```

Yes! You can see, both approaches to do a log-transformation resulted in a more-or-less normal distribution of our data.

Of course, for a log-transformation this recipe-routine may seem a bit over-the-top. But depending on your data and the distribution that is needed for your model to work, different transformations can be done. For example, the log-transformation does not necessarily result in a perfect normal distribution. Therefore, one can use the *Box-Cox* transformation which is less straight-forward to do. Fortunately, the *recipes* package already provides the function `step_BoxCox()`,so that we do not need to implement a *Box-Cox* transformation by hand.

#### Standardization

Several algorithms explicitly require data to be standardized. That is, values of all predictors to vary within a comparable range. The necessity of this step becomes obvious when considering KNN where the distance would be dominated by variables that range over higher absolute values then others. In other words, inputs have to vary over the same range, expecting a mean of zero and standard deviation of one. To get a quick overview of the distribution of all variables (columns) in our data frame, we can use the *skimr* package.

```{r}
library(skimr)
skim(ddf_train)
```

We see for example, that typical values of `LW_IN_F` are by a factor 100 larger than values of `VPD_F`. KNN uses the distance from neighbouring points for predictions. Obviously, in this case here, any distance would be dominated by `LW_IN_F` and distances in the "direction" of `VPD_F`, even when relatively large, would not be influential, neither for a Euclidean nor a Manhattan distance (see \@ref(introduction)). Therefore, it is reasonable to scale distances in a dataset by standardization so that such absolute differences become irrelevant.

Standardization is done by dividing each variable, that is all values in one column, by the standard deviation of that variable, and then subtracting its mean. This way, the resulting standardized values are centered around 0, and scaled such that a value of 1 means that the data point is one standard deviation above the mean of the respective variable (column).

When applied to all predictors individually, the absolute values of their variations can be directly compared and only then it can be meaningfully used for determining the distance. Standardization can be done not only by centering and scaling (as described above), but also by *scaling to within range*, where values are scaled such that the minimum value within each variable (column) is 0 and the maximum is 1.

In order to avoid *data leakage*, centering and scaling has to be done separately for each split into training and validation data (more on that later). In other words, don't center and scale the entire data frame with the mean and standard deviation derived from the entire data frame, but instead center and scale with mean and standard deviation derived from the training portion of the data, and apply that also to the validation portion, when evaluating.

The *caret* package takes care of this. The R package [**caret**](https://topepo.github.io/caret/) provides a unified interface for using different ML algorithms implemented in separate packages. The preprocessing steps applied with each resampling fold can be specified using the function `preProcess()`. More on resampling in Chapter \@ref(training).

```{r warning=FALSE, message=FALSE}
library(caret)
pp <- preProcess(ddf_train, method = c("center", "scale"))
pp
```

As seen above for the feature engineering example, this does not return a standardized version of the data frame `ddf`. Rather, it returns the information that allows us to apply the same standardization also to other data sets. In other words, we use the distribution of values in the data set to which we applied the function to determine the centering and scaling (here: mean and standard deviation). Note that the "ignored" variable is the `TIMESTAMP` which is formatted as a date object and thus cannot be scaled and centered.

Using the *recipes* package, the code goes as follows:

```{r}
pp <- recipe(GPP_NT_VUT_REF ~ ., data = ddf_train) %>% 
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes())
pp
```

Here, we used selectors to apply the recipe step to several variables at once. The first selector, `all_numeric()`, selects all variables that are either integers or real values. The second selector, `-all_outcomes()` removes any outcome (target) variables from this recipe step. As you can see from the output of `pp`, both recipes hold the same information to be applied when training a ML model.

#### Zero-variance predictors [BONUS]

Sometimes, the data generation process yields variables that have the same value in each observation. This can be due to failure of the measurement device or another bug in the data collection pipeline. Either way, this may cause some algorithms to crash or become unstable. Such "zero-variance" predictors are usually removed altogether. The same applies also to variables with "*near*-zero variance". That is, variables where only a few unique values occur in the entire data set with a high frequency. The danger is that, when data is split into training and testing sets, the variable may effectively become a "zero-variance" variable within the training subset.

We can test for zero-variance or near-zero variance predictors by quantifying the following metrics:

-   Frequency ratio: Ratio of the frequency of the most common predictor over the second most common predictor. This should be near 1 for well-behaved predictors and get very large for problematic ones.
-   Percent unique values: The number of unique values divided by the total number of rows in the data set (times 100). For problematic variables, this ratio gets small (approaches 1/100).

The function `nearZeroVar` of the caret package flags suspicious variables (`zeroVar = TRUE` or `nzv = TRUE`). In our data set, we don't find any:

```{r}
nearZeroVar(ddf, saveMetrics= TRUE)
```

Using the *recipes* package, we can add a step that removes zero-variance predictors. Note that the following code picks up the `recipe_example` holding the log-transformation from above and extends the entire recipe by a removal of zero-variance predictors.

```{r}
recipe_example <- recipe_example %>% 
  step_zv(all_predictors())
recipe_example
```

#### One-hot encoding

For ML algorithms that require that all predictors be numerical (e.g., neural networks, or KNN), categorical predictors have to be pre-processed and converted into new numerical predictors. The most common transformation is *one-hot encoding*, where a categorical feature (predictor) with $N$ levels is replaced by $N$ new features that contain either zeros or ones depending whether the value of the categorical feature corresponds to the respective column. This creates perfect co-linearity between these new columns which allows to drop one column. In other words, one needs $N-1$ new features to describe a categorical variable with $N$ levels. This is referred to as *dummy encoding*. Check out Figure \@ref(fig:ohe) for a one-hot encoding visualization.

```{r ohe, echo = F, fig.cap="Visualization of one-hot encoding from [Kaggle](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding)."}
knitr::include_graphics("./figures/06_figure_ohe.png")
```

Let's have a hands-on example to get a better grasp on this. The following code chunk loads the `iris` dataset which holds three different levels for the categorical feature `Species`:

```{r}
library(datasets)
df <- iris %>% 
  dplyr::select(Sepal.Length, Sepal.Width, Species) %>%
  arrange(Sepal.Width)
df %>% 
  head() %>% 
  knitr::kable()
df$Species %>% 
  levels()
```

Now, let's use the handy *recipes* package to turn `Species` into several new columns via the *step_dummy()* command:

```{r}
## To get a feeling for our dataset, we can also print a summary of our recipe
## Note that the formula below does not define any target variable and that the type of 'Species' is nominal
recipe( ~ ., data = df) %>% 
  summary()

## Let's bake our recipe!
baked_df <- recipe( ~ ., data = iris) %>% 
  step_dummy(Species, one_hot = T) %>% 
  prep(., data = df) %>%
  bake(., new_data = df)

baked_df %>% 
  arrange(Sepal.Width) %>%  
  head() %>% 
  knitr::kable()
```

Compare the output of the dataframe before and after baking. Can you see how `Species` has been split into three numerical features and how the describe the different levels? Also, do you understand now, why this process is called 1-hot encoding?

#### Dealing with missingness and bad data

Several ML algorithms require missing values to be removed. That is, if any of the rows has a missing value in at least one cell, the entire row gets removed. Data may be missing for several reasons. Some reasons yield random patterns of missing data, others not. In the latter case, we can speak of *informative missingness* (Kuhn & Johnson, 2003) and its information can be used for predictions. For categorical data, we may replace such data with `"none"` (instead of `NA`), while randomly missing data may be dropped altogether. Some ML algorithms (mainly tree-based methods, e.g., random forest) can handle missing values. However, when comparing the performance of alternative ML algorithms, they should be tested with the same data and removing missing data should be done beforehand.

Visualising missing data is essential for making decisions about dropping rows with missing data versus removing predictors from the model (which would imply too much data removal). The cells with missing data in a data frame can be easily be visualised e.g. with `vis_miss()` from the *visdat* package.

```{r warning=FALSE, message=FALSE}
library(visdat)
vis_miss(
  ddf,
  cluster = FALSE, 
  warn_large_data = FALSE
  )
```

The question about what is "bad data" and whether or when it should be removed is often critical. Such decisions are important to keep track of and should be reported as transparently as possible in publications. In reality, where the data generation process may start in the field with actual human beings writing notes in a lab book, and where the human collecting the data is often not the same as the human writing the paper, it's often more difficult to keep track of such decisions. As a general principle, it is advisable to design data records such that decisions made during its process remain transparent throughout all stages of the workflow and that sufficient information be collected to enable later revisions of particularly critical decisions.

#### More pre-processing

Depending on the algorithm and the data, additional pre-processing steps may be required. You can find more information about this in the great and freely available online tutorial [Hands-On Machine Learning in R](https://bradleyboehmke.github.io/HOML/engineering.html#target-engineering).

One such additional pre-processing step is *imputation*, where missing values are imputed (gap-filled), for example by the mean of each variable respectively. Also imputation is prone to cause data leakage and must therefore be implemented as part of the resampling and training workflow. The *recipes* package offers a great way to deal with imputation (and also all other pre-processing steps). [Here](https://bradleyboehmke.github.io/HOML/engineering.html#impute) is a link to learn more about it.

### Model formulation

The aim of supervised ML is to find a model $\hat{Y} = f(X)$ so that $\hat{Y}$ agrees well with observations $Y$. We typically start with a research question where $Y$ is given - naturally - by the problem we are addressing and we have a data set at hand where one or multiple predictors (or features) $X$ are recorded along with $Y$. From our data, we have information about how GPP (ecosystem-level photosynthesis) depends on set of abiotic factors, mostly meteorological measurements.

#### Formula notation

In R, it is common to use the *formula* notation to specify the target and predictor variables. You have probably encountered formulas before, e.g., for a linear regression using the `lm()` function. To specify a linear regression model for `GPP_NT_VUT_REF` with three predictors `SW_F_IN`, `VPD_F`, and `TA_F`, we write:

```{r eval=F}
lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = ddf)
```

As mentioned throughout this tutorial, the *caret* package provides a unified interface to define different ML models and pre-processing recipes. In the next tutorial, we will learn more about how to do so. In this tutorial's exercise you will deepen your understanding of data wrangling, splitting and pre-processing as well as setting up LM using the formula notation.

## Exercise

Now that you are familiar with the basic steps for supervised machine learning, you can get your hands on the data yourself and implement code for addressing the modelling task outlined in Chapter \@ref(motivation).

### Reading and cleaning

Read the CSV file `"FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv"`, select all variables with name ending with `"_F"`, the variables `"TIMESTAMP"`, `"GPP_NT_VUT_REF"`, and `"NEE_VUT_REF_QC"`, and drop all variables that contain `"JSB"` in their name. Then convert the variable `"TIMESTAMP"` to a date-time object with the function `ymd()` from the *lubridate* package, and interpret all values `-9999` as missing values. Then, set all values of `"GPP_NT_VUT_REF"` to missing if the corresponding quality control variable indicates that less than 90% are measured data points. Finally, drop the variable `"NEE_VUT_REF_QC"` - we won't use it anymore.

```{r}
## write your code here
```

### Data splitting

Split the data into a training and testing set, that contain 70% and 30% of the total available data, respectively.

```{r}
## write your code here
```

### Linear model

#### Training

Fit a linear regression model using the base-R function `lm()` and the training set. The target variable is `"GPP_NT_VUT_REF"`, and predictor variables are all available meterological variables in the dataset. Answer the following questions:

-   What is the $R^2$ of predicted vs. observed `"GPP_NT_VUT_REF"`?
-   Is the linear regression slope significantly different from zero for all predictors?

```{r}
## write your code here
```

#### Prediction

With the model containing all predictors and fitted on `ddf_train`, make predictions using first `ddf_train` and then `ddf_test`. Compute the $R^2$ and the root-mean-square error, and visualise modelled vs. observed values to evaluate both predictions.

Do you expect the linear regression model trained on `ddf_train` to predict substantially better on `ddf_train` than on `ddf_test`? Why (not)?

Hints:

-   To calculate predictions, use the generic function `predict()` with the argument `newdata = ...`.
-   The $R^2$ can be extracted from the model object as `summary(model_object)$r.squared`, or is (as the RMSE) given in the metrics data frame returned by `metrics()` from the *yardstick* library.
-   For visualisation the model performance, consider a scatterplot, or (better) a plot that reveals the density of overlapping points. (We're plotting information from over 4000 data points here!)

```{r}
## write your code here
```

### Pre-processing

To get a better understanding of writing and baking recipes for pre-processing, load the dataset `starwars` from the `dplyr` package. Note that for this part of the exercise you do not need to split the data, just work with the full dataset. Do the following tasks:

**Encoding of factor levels:**

-   Load the dataset and select the features height, mass and species

-   Drop all `NA` entries from the dataset

-   Apply an one-hot encoding and a dummy encoding on the feature species

-   Answer the following questions:

    -   How many columns does the un-processed original data frame have and how many columns are in the freshly "baked" data frames created by the One-hot encoding and how many in the one from the dummy-encoding? Explain the differences.
    -   Which column was created by the One-hot-encoding but not by the dummy-encoding?

```{r}
## write your code here
df <- starwars # tidyverse has to be loaded for this code line!
```

**Sequential pre-processing:**

-   Load the dataset and select the features height, mass and species

-   Drop all `NA` entries from the dataset

-   Write and bake a recipe with the following steps:

    -   Filter out zero or near-zero variance features
    -   Standardize (center and scale) numeric features
    -   Dummy encode categorical features.

-   Visualise the distribution of the numerical variables

-   Answer the question: Does the order in which pre-processing steps are defined in the recipe matter?

```{r}
## write your code here
```
